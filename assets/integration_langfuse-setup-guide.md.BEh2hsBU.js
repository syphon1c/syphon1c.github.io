import{_ as t}from"./chunks/integrations-settings-view-01.DCPNEnSs.js";import{_ as a,o as i,c as s,ah as n}from"./chunks/framework.504LQ1l3.js";const f=JSON.parse('{"title":"Langfuse Integration Setup Guide","description":"","frontmatter":{},"headers":[],"relativePath":"integration/langfuse-setup-guide.md","filePath":"integration/langfuse-setup-guide.md"}'),o={name:"integration/langfuse-setup-guide.md"};function l(r,e,u,g,c,p){return i(),s("div",null,[...e[0]||(e[0]=[n('<h1 id="langfuse-integration-setup-guide" tabindex="-1">Langfuse Integration Setup Guide <a class="header-anchor" href="#langfuse-integration-setup-guide" aria-label="Permalink to &quot;Langfuse Integration Setup Guide&quot;">​</a></h1><h2 id="langfuse-overview" tabindex="-1">Langfuse Overview <a class="header-anchor" href="#langfuse-overview" aria-label="Permalink to &quot;Langfuse Overview&quot;">​</a></h2><p>Langfuse is an open-source LLM observability platform that provides specialized monitoring, prompt management, and analytics for LLM applications. This guide walks you through setting up Langfuse integration with the AI Security Gateway.</p><p><strong>Important</strong>: Langfuse telemetry (tracing) operates <strong>independently</strong> of audit logging. This means:</p><ul><li>✅ Traces are sent to Langfuse even if audit logging is disabled for a proxy</li><li>✅ You can disable audit logs for performance/compliance while maintaining observability</li><li>✅ Audit logs (database) and traces (Langfuse) have separate retention policies</li><li>✅ Use Langfuse for monitoring without storing detailed audit logs locally</li></ul><h2 id="why-use-langfuse" tabindex="-1">Why Use Langfuse? <a class="header-anchor" href="#why-use-langfuse" aria-label="Permalink to &quot;Why Use Langfuse?&quot;">​</a></h2><p>Langfuse provides several advantages over generic observability tools:</p><ul><li><strong>LLM-Specific UI</strong>: Pre-built dashboards designed for LLM workflows</li><li><strong>Prompt Management</strong>: Version control and A/B testing for prompts</li><li><strong>Cost Analysis</strong>: Detailed cost breakdowns per conversation and feature</li><li><strong>Quality Metrics</strong>: Evaluation workflows and feedback collection</li><li><strong>Session Tracking</strong>: Multi-turn conversation tracking and analytics</li></ul><h2 id="langfuse-prerequisites" tabindex="-1">Langfuse Prerequisites <a class="header-anchor" href="#langfuse-prerequisites" aria-label="Permalink to &quot;Langfuse Prerequisites&quot;">​</a></h2><ul><li>AI Security Gateway installed and running</li><li>Langfuse account (cloud) or self-hosted Langfuse instance</li><li>Langfuse project with API keys</li></ul><h2 id="quick-setup-web-ui" tabindex="-1">Quick Setup (Web UI) <a class="header-anchor" href="#quick-setup-web-ui" aria-label="Permalink to &quot;Quick Setup (Web UI)&quot;">​</a></h2><h3 id="step-1-get-langfuse-api-keys" tabindex="-1">Step 1: Get Langfuse API Keys <a class="header-anchor" href="#step-1-get-langfuse-api-keys" aria-label="Permalink to &quot;Step 1: Get Langfuse API Keys&quot;">​</a></h3><p><strong>For Langfuse Cloud:</strong></p><ol><li>Go to <a href="https://cloud.langfuse.com" target="_blank" rel="noreferrer">https://cloud.langfuse.com</a></li><li>Sign up for a free account (if you don&#39;t have one)</li><li>Create a new project or select an existing project</li><li>Navigate to <strong>Settings → API Keys</strong></li><li>Copy your <strong>Public Key</strong> (starts with <code>pk-lf-...</code>)</li><li>Copy your <strong>Secret Key</strong> (starts with <code>sk-lf-...</code>)</li><li><strong>Important</strong>: Keep your secret key secure - never expose it in client-side code</li></ol><p><strong>For Self-Hosted Langfuse:</strong></p><ol><li>Access your Langfuse instance (see <a href="#self-hosted-langfuse-optional">Self-Hosted Setup</a> below)</li><li>Create a project</li><li>Navigate to <strong>Settings → API Keys</strong></li><li>Copy your Public and Secret keys</li></ol><h3 id="step-2-configure-in-ai-security-gateway-web-ui" tabindex="-1">Step 2: Configure in AI Security Gateway Web UI <a class="header-anchor" href="#step-2-configure-in-ai-security-gateway-web-ui" aria-label="Permalink to &quot;Step 2: Configure in AI Security Gateway Web UI&quot;">​</a></h3><ol><li>Open the AI Security Gateway Web UI (default: <a href="http://localhost:8080" target="_blank" rel="noreferrer">http://localhost:8080</a>)</li><li>Navigate to <strong>Settings → Integrations</strong></li><li>Locate the <strong>Langfuse Integration</strong> section</li><li>Enable the integration toggle</li><li>Configure the following settings: <ul><li><strong>Endpoint</strong>: Langfuse endpoint URL <ul><li>For Langfuse Cloud: <code>https://cloud.langfuse.com</code></li><li>For self-hosted (standard OTLP port): <code>http://langfuse:4318</code></li><li>For self-hosted (web interface port): <code>http://langfuse:3000</code></li></ul></li><li><strong>Public Key</strong>: Paste your Langfuse public key (pk-lf-...)</li><li><strong>Secret Key</strong>: Paste your Langfuse secret key (sk-lf-...)</li></ul></li><li>Click <strong>Test Connection</strong> to verify the configuration</li><li>Click <strong>Save Settings</strong> to apply changes</li></ol><p>The integration is now active - no restart required!</p><p><img src="'+t+`" alt="Integration Settings Interface"><em>Configure Langfuse integration through the Web UI Settings → Integrations page</em></p><h2 id="self-hosted-langfuse-optional" tabindex="-1">Self-Hosted Langfuse (Optional) <a class="header-anchor" href="#self-hosted-langfuse-optional" aria-label="Permalink to &quot;Self-Hosted Langfuse (Optional)&quot;">​</a></h2><p>For self-hosted deployments, you can run Langfuse on your own infrastructure.</p><h3 id="deploy-langfuse" tabindex="-1">Deploy Langfuse <a class="header-anchor" href="#deploy-langfuse" aria-label="Permalink to &quot;Deploy Langfuse&quot;">​</a></h3><p>Follow the <a href="https://langfuse.com/docs/deployment/self-host" target="_blank" rel="noreferrer">Langfuse self-hosting guide</a> to deploy Langfuse.</p><h3 id="otlp-endpoint-configuration" tabindex="-1">OTLP Endpoint Configuration <a class="header-anchor" href="#otlp-endpoint-configuration" aria-label="Permalink to &quot;OTLP Endpoint Configuration&quot;">​</a></h3><p>Langfuse accepts OTLP traces via the <code>/api/public/otel/v1/traces</code> endpoint. The gateway automatically formats the endpoint path based on the port you specify:</p><ul><li><p><strong>Standard OTLP port (4318)</strong>: Uses default path <code>/v1/traces</code></p><ul><li>Example: <code>http://langfuse:4318</code> → <code>http://langfuse:4318/v1/traces</code></li></ul></li><li><p><strong>Custom port (e.g., 3000)</strong>: Appends <code>/api/public/otel/v1/traces</code></p><ul><li>Example: <code>http://langfuse:3000</code> → <code>http://langfuse:3000/api/public/otel/v1/traces</code></li></ul></li></ul><h3 id="configure-in-web-ui" tabindex="-1">Configure in Web UI <a class="header-anchor" href="#configure-in-web-ui" aria-label="Permalink to &quot;Configure in Web UI&quot;">​</a></h3><p>Once your self-hosted Langfuse is running:</p><ol><li>Get your API keys from your Langfuse instance (<strong>Settings → API Keys</strong>)</li><li>Follow the <a href="#step-2-configure-in-ai-security-gateway-web-ui">Web UI configuration steps</a> above</li><li>Use your self-hosted endpoint URL (e.g., <code>http://langfuse:4318</code> or <code>http://langfuse:3000</code>)</li></ol><hr><h2 id="testing-the-integration" tabindex="-1">Testing the Integration <a class="header-anchor" href="#testing-the-integration" aria-label="Permalink to &quot;Testing the Integration&quot;">​</a></h2><h3 id="via-web-ui-recommended" tabindex="-1">Via Web UI (Recommended) <a class="header-anchor" href="#via-web-ui-recommended" aria-label="Permalink to &quot;Via Web UI (Recommended)&quot;">​</a></h3><ol><li>Navigate to <strong>Settings → Integrations</strong></li><li>Locate the <strong>Langfuse</strong> integration section</li><li>Click <strong>Test Connection</strong> to verify the configuration</li><li>Review the test results: <ul><li>✅ <strong>Success</strong>: Configuration is valid and endpoint is reachable</li><li>❌ <strong>Failed</strong>: Check the error message and verify your settings</li></ul></li></ol><h3 id="via-test-request" tabindex="-1">Via Test Request <a class="header-anchor" href="#via-test-request" aria-label="Permalink to &quot;Via Test Request&quot;">​</a></h3><p>Send a test LLM request through your proxy to verify traces are being sent:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -X</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> POST</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> http://localhost:8080/proxy/llm/v1/chat/completions</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Authorization: Bearer your-api-key&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;model&quot;: &quot;gpt-4&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;messages&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello, this is a test message&quot;}</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    ]</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  }&#39;</span></span></code></pre></div><p>Then check your Langfuse UI (Traces section) - you should see a trace appear within a few seconds.</p><hr><h2 id="viewing-traces-in-langfuse" tabindex="-1">Viewing Traces in Langfuse <a class="header-anchor" href="#viewing-traces-in-langfuse" aria-label="Permalink to &quot;Viewing Traces in Langfuse&quot;">​</a></h2><p>After making requests through your LLM proxies:</p><ol><li>Open your Langfuse project in a browser</li><li>Navigate to <strong>Traces</strong> section</li><li>You should see a trace for your test request within a few seconds</li><li>Click on the trace to view details: <ul><li>Request/response data</li><li>Token usage</li><li>Cost information</li><li>Security context (risk level, violations)</li><li>Model parameters</li></ul></li></ol><h2 id="using-langfuse-features" tabindex="-1">Using Langfuse Features <a class="header-anchor" href="#using-langfuse-features" aria-label="Permalink to &quot;Using Langfuse Features&quot;">​</a></h2><h3 id="viewing-traces" tabindex="-1">Viewing Traces <a class="header-anchor" href="#viewing-traces" aria-label="Permalink to &quot;Viewing Traces&quot;">​</a></h3><ol><li><strong>Navigate to Traces</strong>: Click &quot;Traces&quot; in the Langfuse sidebar</li><li><strong>Filter Traces</strong>: Use filters to find specific traces: <ul><li>By model</li><li>By proxy name</li><li>By risk level</li><li>By date range</li></ul></li><li><strong>View Details</strong>: Click on a trace to see: <ul><li>Full prompt and response</li><li>Token usage breakdown</li><li>Cost calculation</li><li>Security metadata</li><li>Performance metrics</li></ul></li></ol><h3 id="analyzing-costs" tabindex="-1">Analyzing Costs <a class="header-anchor" href="#analyzing-costs" aria-label="Permalink to &quot;Analyzing Costs&quot;">​</a></h3><ol><li><strong>Navigate to Analytics</strong>: Click &quot;Analytics&quot; in the sidebar</li><li><strong>View Cost Breakdown</strong>: See costs by: <ul><li>Model</li><li>Proxy</li><li>Time period</li><li>User/team</li></ul></li><li><strong>Set Budgets</strong>: Configure budget alerts in Langfuse</li></ol><h3 id="prompt-management" tabindex="-1">Prompt Management <a class="header-anchor" href="#prompt-management" aria-label="Permalink to &quot;Prompt Management&quot;">​</a></h3><ol><li><strong>Navigate to Prompts</strong>: Click &quot;Prompts&quot; in the sidebar</li><li><strong>Create Prompt Templates</strong>: Define reusable prompt templates</li><li><strong>Version Control</strong>: Track prompt changes over time</li><li><strong>A/B Testing</strong>: Compare different prompt versions</li></ol><h3 id="evaluations" tabindex="-1">Evaluations <a class="header-anchor" href="#evaluations" aria-label="Permalink to &quot;Evaluations&quot;">​</a></h3><ol><li><strong>Navigate to Evaluations</strong>: Click &quot;Evaluations&quot; in the sidebar</li><li><strong>Create Scorecards</strong>: Define evaluation criteria</li><li><strong>Run Evaluations</strong>: Automatically evaluate traces</li><li><strong>Collect Feedback</strong>: Gather human feedback on responses</li></ol><h2 id="langfuse-next-steps" tabindex="-1">Langfuse Next Steps <a class="header-anchor" href="#langfuse-next-steps" aria-label="Permalink to &quot;Langfuse Next Steps&quot;">​</a></h2><p>After setting up Langfuse:</p><ol><li><strong>Explore Traces</strong>: Review traces in Langfuse UI to understand data structure</li><li><strong>Set Up Dashboards</strong>: Create custom dashboards for your use cases</li><li><strong>Configure Alerts</strong>: Set up alerts for cost, errors, or security issues</li><li><strong>Integrate Evaluations</strong>: Set up evaluation workflows for quality monitoring</li><li><strong>Manage Prompts</strong>: Create and version prompt templates</li></ol><h2 id="langfuse-resources" tabindex="-1">Langfuse Resources <a class="header-anchor" href="#langfuse-resources" aria-label="Permalink to &quot;Langfuse Resources&quot;">​</a></h2><ul><li><a href="https://langfuse.com/docs" target="_blank" rel="noreferrer">Langfuse Documentation</a></li><li><a href="https://api.reference.langfuse.com/#tag/opentelemetry/post/apipublicotelv1traces" target="_blank" rel="noreferrer">Langfuse OpenTelemetry API Reference</a></li><li><a href="https://cloud.langfuse.com" target="_blank" rel="noreferrer">Langfuse Cloud</a></li><li><a href="https://opentelemetry.io/docs/" target="_blank" rel="noreferrer">OpenTelemetry Documentation</a></li><li><a href="./../operations/observability-user-guide.html">AI Security Gateway Observability Guide</a></li></ul><h2 id="langfuse-support" tabindex="-1">Langfuse Support <a class="header-anchor" href="#langfuse-support" aria-label="Permalink to &quot;Langfuse Support&quot;">​</a></h2><p>If you encounter issues:</p><ol><li>Check this guide&#39;s troubleshooting section</li><li>Review gateway logs for errors</li><li>Check Langfuse documentation</li><li>Open an issue on the gateway GitHub repository</li></ol>`,59)])])}const y=a(o,[["render",l]]);export{f as __pageData,y as default};
