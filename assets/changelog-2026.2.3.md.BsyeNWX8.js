import{_ as e,o as a,c as n,ah as o}from"./chunks/framework.504LQ1l3.js";const p=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"changelog-2026.2.3.md","filePath":"changelog-2026.2.3.md"}'),i={name:"changelog-2026.2.3.md"};function r(s,t,l,c,d,u){return a(),n("div",null,[...t[0]||(t[0]=[o('<h2 id="_2026-2-3-beta" tabindex="-1">[2026.2.3-beta] <a class="header-anchor" href="#_2026-2-3-beta" aria-label="Permalink to &quot;[2026.2.3-beta]&quot;">‚Äã</a></h2><h3 id="üéâ-third-public-beta-release" tabindex="-1">üéâ Third Public Beta Release <a class="header-anchor" href="#üéâ-third-public-beta-release" aria-label="Permalink to &quot;üéâ Third Public Beta Release&quot;">‚Äã</a></h3><p>The <strong>AI Security Gateway</strong> is a unified security platform providing real-time monitoring, policy enforcement, and threat detection for Large Language Model (LLM) APIs and Model Context Protocol (MCP) servers. This beta release represents a comprehensive security proxy and monitoring platform for AI infrastructure.</p><p>This release introduces our Guardrails Evaluation scanning tool, improved relationship visualisation graphs on the dashboard, and enhancements to the Canary Token detection feature.</p><hr><h4 id="üïµÔ∏è-canary-token-detection" tabindex="-1">üïµÔ∏è Canary Token Detection <a class="header-anchor" href="#üïµÔ∏è-canary-token-detection" aria-label="Permalink to &quot;üïµÔ∏è Canary Token Detection&quot;">‚Äã</a></h4><p>Canary Token Injection is a security feature that helps detect when data from one user or session is accidentally exposed to another. Think of it as a tripwire : an early warning system that alerts you to potential data leakage in your AI systems.</p><p>When proxying requests, the gateway silently injects unique, invisible tokens into each user&#39;s conversation. If a token surfaces where it shouldn&#39;t, you&#39;ll know immediately.</p><p><strong>Detection types:</strong></p><ul><li><strong>Cross-User Leakage</strong>: A canary from User A appeared in a response to User B, indicating data bleed between users</li><li><strong>Cross-Session Leakage</strong>: A canary from Session A appeared in the same user&#39;s Session B, indicating session isolation failure</li><li><strong>Provider Memorisation</strong>: A canary resurfaced without being present in the current context, suggesting the LLM provider has memorised prior conversation data</li><li><strong>Stale Canary</strong>: A canary older than 7 days reappeared, a strong indicator of long-term memorisation by the model provider</li></ul><hr><h4 id="üõ°Ô∏è-guardrails-evaluation" tabindex="-1">üõ°Ô∏è Guardrails Evaluation <a class="header-anchor" href="#üõ°Ô∏è-guardrails-evaluation" aria-label="Permalink to &quot;üõ°Ô∏è Guardrails Evaluation&quot;">‚Äã</a></h4><p>Having guardrails is one thing. Knowing they actually work is another.</p><p>Guardrails Evaluation is automated penetration testing for your AI safety controls. It runs a comprehensive suite of security test cases against your endpoints and scores the results against the <strong>OWASP LLM Top 10</strong> and <strong>NIST AI Risk Management Framework</strong>.</p><p><strong>What it tests:</strong></p><ul><li><strong>Prompt Injection</strong>: Direct injection, goal hijacking, and system prompt extraction</li><li><strong>MCP Tool Poisoning</strong>: Malicious tool descriptions, command injection, and protocol exploitation</li><li><strong>Bypass Techniques</strong>: Encoding obfuscation, flag manipulation, and filter evasion</li><li><strong>Semantic &amp; Structural Evasion</strong>: Skeleton key, roleplay, payload splitting, homoglyph substitution, and multilingual evasion</li><li><strong>Data Exfiltration</strong>: Credential theft, PII extraction, and proprietary data leakage</li><li><strong>Multi-Turn Escalation</strong>: Crescendo attacks, echo chamber context poisoning, and many-shot overrides</li><li><strong>Harmful Content &amp; Toxicity</strong>: Requests to generate violent, self-harm, or weapons content</li><li><strong>Misinformation</strong>: Fake news generation and disinformation campaigns</li><li><strong>PII Extraction</strong>: Attempts to extract personal data about real individuals</li><li><strong>Resource Exhaustion</strong>: Recursive loops, context window flooding, and token bombing</li><li><strong>Benign Controls</strong>: Legitimate requests that should pass through (false positive validation)</li></ul><p><strong>Key features:</strong></p><ul><li><strong>80+ built-in test cases</strong> across 12 categories, with the ability to add your own custom tests</li><li><strong>Compliance scoring</strong> mapped to OWASP LLM Top 10 and NIST AI RMF</li><li><strong>Test any endpoint</strong>: works with any API that wraps an LLM, not just direct LLM providers. Import endpoints via curl command paste</li><li><strong>Multi-turn attack simulation</strong>: tests that span multiple conversation turns to detect escalation vulnerabilities</li><li><strong>Per-category risk breakdown</strong> with pass/fail rates and weighted risk scores</li></ul><hr><h4 id="üîó-relationship-visualisation" tabindex="-1">üîó Relationship Visualisation <a class="header-anchor" href="#üîó-relationship-visualisation" aria-label="Permalink to &quot;üîó Relationship Visualisation&quot;">‚Äã</a></h4><p>A new interactive graph on the dashboard that visualises relationships between proxies, policies, users, and data flows, making it easier to understand how your AI infrastructure is connected at a glance.</p>',21)])])}const h=e(i,[["render",r]]);export{p as __pageData,h as default};
